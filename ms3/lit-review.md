
# Literature Review
## Jian Hu, Lu Xu, Zhengyuan Xu, Xingjian Sun

The automation of commonsense reasoning based on the machine commonsense knowledge plays a critical role in natural language processing (NLP). Many of the domains involved in commonsense inference are only partially understood or generally untouched. A range of challenges and difficulties along with commonsense reasoning is discussed by Davis and Marcus (2015), which covers from problems on formulating commonsense knowledge for specific genres to complexities of various form integrations for problem-solving. To innovate this field, there is a clear need for methods given with relevant document or context sentences such that information retrieval (IR) models with various attention-based architectures can achieve high performance, comparable to human performance, on such datasets. Training the machine to have the capacity of human processing, spans a large portion of human experience, encompassing a rich understanding of all the aspects of typical everyday life. In the NLP community, a plausible account of research activities surges and attempts to address commonsense inference through implementing different models and evaluate the performance based on different benchmark tasks. However, in settings like dialogue generation or recommendation systems, the complete set of relevant contexts may not be provided, which makes commonsense knowledge important for general natural language understanding. 

In line with present deep learning machinery, a first action to be taken for grounded commonsense reasoning is to create a vast number of items in a large-scale dataset. Situations with Adversarial Generations (SWAG) was introduced by Zellers et al. (2018) to automatically detect and reduce stylistic artifacts. This is an adversarial dataset with 113k multiple-choice questions (73k training, 20k validation, 20k test) constructing extensive empirical results for natural language inference and commonsense reasoning. The dataset is generated from pairs of video captions where they slightly differ from each other in nature in order to have a wide-ranging coverage, and finally, each question has a human-verified ending and three distractors. SWAG takes the form of natural language inference, where a complete context sentence describing a scene is given, and the task is to choose out of four candidate sentences the one most likely to describe the next scene in a video. To minimize the annotation artifacts, the dataset diversity is embedded in SWAG but also coordinates conditional stylistic patterns, for instance, length and word-preference biases. It outperforms many nature language inference datasets because a novel procedure of Adversarial Filtering (AF) proposed which frames a de-biased dataset through training an ensemble of stylistic classifiers, and using them to filter the data.  This enables shallow models such as bag-of-words to receive artificially high performance. SWAG is publicly available and can be accessed via https://github.com/rowanz/swagaf.

Moreover, another dataset CommonsenseQA focused on commonsense question-answering was proposed by Talmor et al. (2018). The development of this method advances the capacity of generating questions that can be easily answered by humans without context, and requires commonsense knowledge. The dataset is derived from a graph-knowledge base, each with one source concept and three target concepts. The crowdsourcing workers were invited to verify the questionsâ€™ quality and then each question was added with a textural context by means of interrogating a search engine and retrieving web snippets. It is overall consisted of 12,247 examples emphasized around commonsense and generating vast amount of questions at scale. The dataset can be downloaded from www.tau-nlp.org/commonsenseqa. 

Datasets that focusing a specific area of study has also been proposed. Mihaylov et al. (2018) presented a kind of question answering dataset, OpenBookQA. The dataset modeled after open book exams for assessing human understanding of a subject. Consisting of around 6,000 questions derived from 1326 elementary level science facts, the dataset requires a combination of an open book fact with broad common knowledge (e.g., a suit of armor is made of metal) obtained from other sources. OpenBookQA probes a deeper understanding of the context of common knowledge and the language it is expressed in. Human performance on this dataset is nearly 92% whereas many pre-trained QA methods failed to achieve an accuracy anywhere close to that. The dataset can be found at https://leaderboard.allenai.org/open_book_qa/submissions/public.

This project takes these datasets that both aim at encouraging candidate models to integrate and understand common sense knowledge. SWAG asks models to select the correct description of what happens next after initializing an event and CommonsenseQA is a concept-based question answering dataset, where one question sentence is given, and the task is to choose out of five candidates the one concept that is the best possible answer based on common sense. 

![alt text](https://github.com/saaamxzy/BERT_with_SWAG/blob/master/ms3/fig1.png?raw=true)

A general pipeline (Storks, et al., 2019) for neural network based commonsense natural language task consists of feature extraction using language representations, fine-tuning the features using RNN or CNN based architectures, and task-specific output layers for prediction. For language inference datasets like SWAG, one approach for the feature fine-tuning is to measure the entailment score using attention mechanism. ESIM model (Chen et al., 2016), as an example, takes word embeddings (e.g. GloVe or ELMo) as input, encodes the input context and candidate answer sequences separately with bidirectional LSTM or Tree-LSTM,  applies word-to-word attention between the two outputs of the encoder, and makes classification using another layer of BiLSTM/Tree-LSTM and a multilayer perceptron classifier. 

![alt text](https://github.com/saaamxzy/BERT_with_SWAG/blob/master/ms3/fig2.png?raw=true)

In addition to datasets-chosen, a language representation model called BERT (Devlin et al., 2018) which abbreviates for Bidirectional Encoder Representations from Transformers will be implemented in this project. BERT tackles the unidirectional constraints by proposing a new pre-training objective. BERT cooperates masked language models to empower pre-trained deep bidirectional representations. It becomes the first fine-turning based language representation model that progresses to state-of-the-art performance on a large quantity of sentence-level and token-level tasks, performing better than many systems focused on task-specific architectures. The attempt of implementing BERT in commonsense inference this project should be a proper embedding method for these tasks, since it can capture the context semantic information from pre-training on large-scale corpora.

## References

Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.

Davis, E., & Marcus, G. (2015). Commonsense reasoning and commonsense knowledge in artificial intelligence. Commun. ACM, 58(9), 92-103.
Talmor, A., Herzig, J., Lourie, N., & Berant, J. (2018). CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. arXiv preprint arXiv:1811.00937.

Storks, S., Gao, Q., & Chai, J. Y. (2019). Commonsense Reasoning for Natural Language Understanding: A Survey of Benchmarks, Resources, and Approaches. arXiv preprint arXiv:1904.01172.

Zellers, R., Bisk, Y., Schwartz, R., & Choi, Y. (2018). Swag: A large-scale adversarial dataset for grounded commonsense inference. arXiv preprint arXiv:1808.05326.

Mihaylov, T., Clark, P., Khot, T., Sabharwal, A. Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering. arXiv preprint arXiv: 1809.02789.


## Slides Link

https://docs.google.com/presentation/d/1dJhBMad3pHM4Ws4tQ0sXq82kEpMS-XQ0q5jc4-V0L5k/edit?usp=sharing